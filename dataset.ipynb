{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22496592-291c-4923-8e11-373e724e95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mkh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/mkh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk_utils import tokenize_fn, stem_fn ,bag_of_words\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from classes import createChatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44860f29-2ecd-496c-b029-ce40bd227ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', \"r\") as f:\n",
    "    intents = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a4422d-ed95-45ab-93a9-9f7885a9efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization -> Lower+stem --> exclude punctuation --> bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c12a21c-6f25-4cb0-9569-d114690051c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Hey', 'How', 'you', 'Is', 'anyone', 'there', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'Thank', \"'s\", 'lot', 'Which', 'items', 'you', 'have', 'What', 'kinds', 'of', 'items', 'there', 'What', 'you', 'sell', 'Do', 'you', 'take', 'credit', 'cards', 'Do', 'you', 'accept', 'Mastercard', 'Can', 'pay', 'with', 'Paypal', 'Are', 'you', 'cash', 'only', 'How', 'long', 'does', 'delivery', 'take', 'How', 'long', 'does', 'shipping', 'take', 'When', 'get', 'my', 'delivery', 'Tell', 'me', 'joke', 'Tell', 'me', 'something', 'funny', 'Do', 'you', 'know', 'joke']\n",
      "['hi', 'hey', 'how', 'you', 'is', 'anyon', 'there', 'hello', 'good', 'day', 'bye', 'see', 'you', 'later', 'goodby', 'thank', 'thank', 'you', 'that', \"'s\", 'help', 'thank', \"'s\", 'lot', 'which', 'item', 'you', 'have', 'what', 'kind', 'of', 'item', 'there', 'what', 'you', 'sell', 'do', 'you', 'take', 'credit', 'card', 'do', 'you', 'accept', 'mastercard', 'can', 'pay', 'with', 'paypal', 'are', 'you', 'cash', 'onli', 'how', 'long', 'doe', 'deliveri', 'take', 'how', 'long', 'doe', 'ship', 'take', 'when', 'get', 'my', 'deliveri', 'tell', 'me', 'joke', 'tell', 'me', 'someth', 'funni', 'do', 'you', 'know', 'joke']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mkh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "for intent in intents[\"intents\"] :\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns'] :\n",
    "        w = tokenize_fn(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w,tag))\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords, punctuation, and ignore list\n",
    "stop_words =[\" \"] # set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# The ignore list containing words to exclude\n",
    "ignore_list = [\n",
    "    \"test\", \"example\", \"random\", \"blah\", \"lorem ipsum\", \"qwerty\", \"asdf\", \"xyz\", \"123\", \n",
    "    \"password\", \"admin\", \"test123\", \"fake\", \"spam\", \"no response\", \"undefined\", \"null\", \n",
    "    \"help\", \"not available\", \"nothing\", \"undefined input\", \"unknown\", \"this is a test\", \n",
    "    \"demo\", \"irrelevant\", \"unsupported\", \"error\", \"fail\", \"sorry\", \"not applicable\", \",\", \n",
    "    \".\", \"!\", \"?\", \";\", \":\", \"-\", \"_\", \"'\", \"\\\"\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \n",
    "    \"&\", \"%\", \"$\", \"#\", \"@\", \"/\", \"\\\\\", \"~\", \"^\", \"*\", \"|\", \"+\", \"=\" , \"it\", \"'m\", \"a\", \"an\", \n",
    "    \"the\", \"are\", \"or\", \"be\", \"do\", \"I\"\n",
    "]\n",
    "\n",
    "# Assuming stem_fn is defined to stem words (e.g., using nltk.stem)\n",
    "from nltk.stem import PorterStemmer\n",
    "stem_fn = PorterStemmer().stem\n",
    "\n",
    "# Filter out stopwords, punctuation, and ignore list\n",
    "filtered_words = [word for word in all_words if word not in stop_words and word not in punctuation and word not in ignore_list]\n",
    "print(filtered_words)\n",
    "# Apply stemming\n",
    "all_words = [stem_fn(w) for w in filtered_words]\n",
    "\n",
    "# Output the processed words\n",
    "print(all_words)\n",
    "\n",
    "# print(all_words)\n",
    "all_words = sorted(set(all_words))\n",
    "# print(all_words)\n",
    "tags = sorted(set(tags))\n",
    "# print(tags)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6863f389-cc82-4e9c-a27c-1dd345217ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 patterns \n",
      "\n",
      "7 tags:\n",
      " ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n",
      "52 all words:\n",
      " [\"'s\", 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'my', 'of', 'onli', 'pay', 'paypal', 'see', 'sell', 'ship', 'someth', 'take', 'tell', 'thank', 'that', 'there', 'what', 'when', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(len(xy) , \"patterns \\n\" )\n",
    "print(len(tags), \"tags:\\n\", tags)\n",
    "print(len(all_words),\"all words:\\n\", all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60ff5dc-dae1-465e-83be-8319bf0e8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Hey']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['How', 'are', 'you']\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "['Is', 'anyone', 'there', '?']\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Hello']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Good', 'day']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Bye']\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['See', 'you', 'later']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "['Goodbye']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Thanks']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Thank', 'you']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "['That', \"'s\", 'helpful']\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Thank', \"'s\", 'a', 'lot', '!']\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Which', 'items', 'do', 'you', 'have', '?']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1.]\n",
      "['What', 'kinds', 'of', 'items', 'are', 'there', '?']\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0.]\n",
      "['What', 'do', 'you', 'sell', '?']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1.]\n",
      "['Do', 'you', 'take', 'credit', 'cards', '?']\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "['Do', 'you', 'accept', 'Mastercard', '?']\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "['Can', 'I', 'pay', 'with', 'Paypal', '?']\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0.]\n",
      "['Are', 'you', 'cash', 'only', '?']\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "['How', 'long', 'does', 'delivery', 'take', '?']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['How', 'long', 'does', 'shipping', 'take', '?']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['When', 'do', 'I', 'get', 'my', 'delivery', '?']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0.]\n",
      "['Tell', 'me', 'a', 'joke', '!']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Tell', 'me', 'something', 'funny', '!']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['Do', 'you', 'know', 'a', 'joke', '?']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "## bag of the words \n",
    "# create training data\n",
    "X_train = []\n",
    "y_train = [] \n",
    "\n",
    "for (pattern_sentence , tag) in xy:\n",
    "    print(pattern_sentence)\n",
    "    bag =  bag_of_words(pattern_sentence, all_words)\n",
    "    print(bag)\n",
    "    X_train.append(bag)\n",
    "\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label) # for calculate the loss_fn -- CreossEntropy\n",
    "    classes = tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656ef2c5-f46e-4df5-bac1-b1699e5f043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]], dtype=float32),\n",
       " array([3, 3, 3, 3, 3, 3, 2, 2, 2, 6, 6, 6, 6, 4, 4, 4, 5, 5, 5, 5, 0, 0,\n",
       "        0, 1, 1, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94336967-552e-4088-ab87-07c08ce9a410",
   "metadata": {},
   "source": [
    "### Create the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09edf74c-3067-4c9c-b542-9ec8bbfe4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = createChatDataset(X_train,y_train,classes)\n",
    "\n",
    "print(len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d20903-c25b-4557-a332-35c4adc89ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b0a188-f16a-4810-9b07-8d973d4c0b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 2, 2, 2, 6, 6, 6, 6, 4, 4, 4, 5, 5, 5, 5, 0, 0,\n",
       "       0, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54c199-dbc9-403c-a866-a76d07d27f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6998579d-343b-4742-9350-cb7c957208b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, \"train_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d8840b-c410-46ef-8199-6ebf1a01e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
